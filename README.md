# Personalized Anomaly Detection

## Description

This project aims to develop a personalized anomaly detection system using a Neural Stochastic Multi-Modal Point Process (NSMMPP). The goal is to model and detect irregular events in temporal sequences in a way that can be personalized for individual sequences or users.

## Features
- Point process-based event modeling using NSMMPP
- Evaluation metrics including ROC curves and AUC scores
- Easily configurable through YAML files
- Experiment tracking using Aim

## Project Organization
------------

    ├── README.md          <- The top-level README for developers using this project.
    │
    ├── configs            <- Configs files
    |   ├── dataset_config.yaml
    |   ├── main_config.yaml
    |   ├── predict_config.yaml
    │   └── train_config.yaml          
    |
    ├── data
    │   └── raw            <- The original, immutable data dump.
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebook
    │   └── data           <- Data which is used in notebooks
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment
    │
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to generate data and create dataloaders
    │   │   │                 
    │   │   ├── dataloader.py
    │   │   └── make_dataset.py
    │   │
    │   ├── entities       <- Dataclasses for efficient developing, based on configs
    │   │   │                 
    │   │   ├── dataset_params.py
    │   │   ├── main_params.py
    │   │   ├── test_params.py
    │   │   └── train_param.py
    │   │
    │   ├── metrics        <- Scripts to calculate final metrics
    │   │   └── calculate_metrics.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── baselines.py
    │   │   ├── cppod.py
    │   │   ├── model_manager.py
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    ├── metrics            <- Csv files with final metrics
    ├── .python-version    <- python version for this project, generated by pyenv
    ├── main.py            <- Python script which contains end-to-end pipeline including aim to track and visualise expirements
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io

--------

## Installation

Install the required packages by running the following command:

```bash
pip install -r requirements.txt
```

## How to Run

### Data Generation

To generate the dataset, run:

```bash
python -m src.data.make_dataset
```

### Training

To train the model, run:

```bash
python -m src.models.train_model
```

### Prediction

To run prediction on a trained model, run:

```bash
python -m src.models.predict_model
```

### Metrics Calculation

To calculate the metrics, run:

```bash
python -m src.metrics.calculate_metrics
```

### End-to-End Pipeline

To run the end-to-end pipeline with Aim experiment tracking, run:

```bash
python main.py
```

## Configuration

You can configure different parts of the project using the YAML files located under the `configs` directory.
